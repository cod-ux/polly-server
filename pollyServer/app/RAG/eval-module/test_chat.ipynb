{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from phoenix.otel import register\n",
    "\n",
    "from ragas.metrics import (\n",
    "    FactualCorrectness,\n",
    "    Faithfulness,\n",
    "    SemanticSimilarity,\n",
    "    LLMContextRecall,\n",
    ")\n",
    "from ragas import evaluate\n",
    "from ragas import EvaluationDataset\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "secrets_path = os.path.join(BASE_DIR, \"..\", \"..\", \"config\", \"secrets.toml\")\n",
    "docs_folder = os.path.join(BASE_DIR, \"docs\")\n",
    "prompt_folder = os.path.join(BASE_DIR, \"prompts\")\n",
    "\n",
    "queries_path = os.path.join(BASE_DIR, \"..\", \"eval-ds\", \"Queries.xlsx\")\n",
    "output_folder = os.path.join(BASE_DIR, \"..\", \"eval-ds\")\n",
    "\n",
    "API_KEY = toml.load(secrets_path)[\"OPENAI_API_KEY\"]\n",
    "\n",
    "embedding_model = \"text-embedding-3-large\"\n",
    "embeddings = OpenAIEmbeddings(model=embedding_model, api_key=API_KEY)\n",
    "\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "chroma_dir = os.path.join(BASE_DIR, \"..\", \"chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare vector store\n",
    "vector_store = Chroma(\n",
    "    persist_directory=chroma_dir,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"polly-rag\",\n",
    ")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEY\n",
    "\n",
    "# Prepare LLM Judge\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "\n",
    "def query_db(query: str):\n",
    "    results = vector_store.similarity_search(query=query, k=3)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Prepare chat function\n",
    "def generate_response(question):\n",
    "    # Query VectorDB\n",
    "    source = query_db(question)\n",
    "    context = [page.page_content for page in source]\n",
    "    string_context = \"\\n\".join(context)\n",
    "\n",
    "    with open(os.path.join(prompt_folder, \"system.md\"), \"r\", encoding=\"utf-8\") as file:\n",
    "        system_prompt = file.read()\n",
    "\n",
    "    with open(os.path.join(prompt_folder, \"user.md\"), \"r\", encoding=\"utf-8\") as file:\n",
    "        user_prompt = file.read()\n",
    "\n",
    "    response = (\n",
    "        client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt.format(context=string_context),\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": user_prompt.format(question=question)},\n",
    "            ],\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "\n",
    "    return response, context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation\n",
    "# Prepare dataset - List of dicts [{\"reference\": \"\", \"query\": \"\", \"response\": \"\"}...]\n",
    "# Context precision, Context recall, Response Relevancy, Faithfullness\n",
    "\n",
    "# Iterate through queries - make dataframe with reference, query, response\n",
    "\n",
    "\n",
    "def build_eval_ds(queries_df, evals_df):\n",
    "    print(\"Building evaluation dataset...\")\n",
    "    questions = []\n",
    "    answers = []\n",
    "    contexts = []\n",
    "    ground_truth = []\n",
    "    for index, row in queries_df.iterrows():\n",
    "        length = len(queries_df)\n",
    "        print(f\"Working on question: {index+1}/{length}\")\n",
    "        questions.append(row[\"Queries\"])\n",
    "        ground_truth.append(row[\"Ground truth\"])\n",
    "        answer, context = generate_response(row[\"Queries\"])\n",
    "        answers.append(answer)\n",
    "        contexts.append(context)\n",
    "\n",
    "    evals_df[\"user_input\"] = questions\n",
    "    evals_df[\"response\"] = answers\n",
    "    evals_df[\"retrieved_contexts\"] = contexts\n",
    "    evals_df[\"reference\"] = [str(truth) for truth in ground_truth]\n",
    "\n",
    "    evals_dataset = EvaluationDataset.from_pandas(evals_df)\n",
    "    return evals_dataset\n",
    "\n",
    "\n",
    "def run_eval(df, llm=evaluator_llm, emd=evaluator_embeddings):\n",
    "    print(\"Running evaluation...\")\n",
    "    metrics = [\n",
    "        LLMContextRecall(llm=llm),\n",
    "        FactualCorrectness(llm=llm),\n",
    "        Faithfulness(llm=llm),\n",
    "        SemanticSimilarity(embeddings=evaluator_embeddings),\n",
    "    ]\n",
    "    results = evaluate(dataset=df, metrics=metrics).to_pandas()\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building evaluation dataset...\n",
      "Working on question: 1/12\n",
      "Working on question: 2/12\n",
      "Working on question: 3/12\n",
      "Working on question: 4/12\n",
      "Working on question: 5/12\n",
      "Working on question: 6/12\n",
      "Working on question: 7/12\n",
      "Working on question: 8/12\n",
      "Working on question: 9/12\n",
      "Working on question: 10/12\n",
      "Working on question: 11/12\n",
      "Working on question: 12/12\n",
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  27%|██▋       | 13/48 [00:06<00:24,  1.42it/s]Exception raised in Job[17]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n",
      "Evaluating:  29%|██▉       | 14/48 [00:06<00:19,  1.71it/s]Exception raised in Job[9]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n",
      "Evaluating:  52%|█████▏    | 25/48 [00:14<00:14,  1.64it/s]Exception raised in Job[33]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n",
      "Evaluating:  65%|██████▍   | 31/48 [00:16<00:06,  2.66it/s]Exception raised in Job[37]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n",
      "Evaluating: 100%|██████████| 48/48 [00:45<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename exists...\n",
      "Filename is valid...\n",
      "Saved file...\n",
      "Save init-test-1.xlsx successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "eval_df = pd.DataFrame()\n",
    "query_df = pd.read_excel(queries_path)\n",
    "\n",
    "# Initating test\n",
    "\n",
    "response_ds = build_eval_ds(queries_df=query_df, evals_df=eval_df)\n",
    "eval_results = run_eval(response_ds)\n",
    "\n",
    "filename = input(\"Save table as: \")\n",
    "\n",
    "Saved = False\n",
    "while not Saved:\n",
    "    if filename:\n",
    "        print(\"Filename exists...\")\n",
    "        if not os.path.exists(\n",
    "            os.path.join(output_folder, \"output\", f\"{filename}.xlsx\")\n",
    "        ):\n",
    "            print(\"Filename is valid...\")\n",
    "            eval_results.to_excel(\n",
    "                os.path.join(output_folder, \"output\", f\"{filename}.xlsx\"),\n",
    "                index=False,\n",
    "            )\n",
    "            print(\"Saved file...\")\n",
    "            print(f\"Save {filename}.xlsx successfully\")\n",
    "            Saved = True\n",
    "\n",
    "        else:\n",
    "            print(\"File already exists\")\n",
    "\n",
    "    else:\n",
    "        print(\"Please enter a valid name to save the file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>factual_correctness</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>semantic_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the purpose of the \"General Power of C...</td>\n",
       "      <td>The \"General Power of Competence\" was introduc...</td>\n",
       "      <td>The \"General Power of Competence,\" as outlined...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.971981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the causes for concern in England's l...</td>\n",
       "      <td>The causes for concern in England's local gove...</td>\n",
       "      <td>The main concerns with England's local governm...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.975568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What percentage of line managers somewhat disa...</td>\n",
       "      <td>According to the Reform/CSW survey, 41% of lin...</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the Department of Health and Social C...</td>\n",
       "      <td>According to the response to the FOI request, ...</td>\n",
       "      <td>The response to the FOI request is not clear o...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.903955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What percentage of line managers strongly disa...</td>\n",
       "      <td>According to the Reform /CSW survey, 41% of li...</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.762745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What contributed to the Windrush scandal?</td>\n",
       "      <td>The Windrush scandal was primarily contributed...</td>\n",
       "      <td>“Operational and organisational failing” at th...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.784868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the delivery and administrative power...</td>\n",
       "      <td>Local government in England has various delive...</td>\n",
       "      <td>In England, local governments possess specific...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.962029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the primary focus of Reform's new prog...</td>\n",
       "      <td>The primary focus of Reform's new programme, \"...</td>\n",
       "      <td>The primary focus of Reform's new programme, \"...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What percentage of respondents strongly or som...</td>\n",
       "      <td>The provided content does not include informat...</td>\n",
       "      <td>62 per cent</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.783762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What was the rank of the UK in terms of measle...</td>\n",
       "      <td>In 2021, the UK ranked 31st in terms of measle...</td>\n",
       "      <td>31st</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How does Reform define AI?</td>\n",
       "      <td>Reform uses the definition of AI provided by t...</td>\n",
       "      <td>Theories and techniques developed to allow com...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.869919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Is there a risk of a 'race to the bottom' in d...</td>\n",
       "      <td>Yes, there are concerns that more granular dec...</td>\n",
       "      <td>There is a concern that increased decentralisa...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0   What is the purpose of the \"General Power of C...   \n",
       "1   What are the causes for concern in England's l...   \n",
       "2   What percentage of line managers somewhat disa...   \n",
       "3   How does the Department of Health and Social C...   \n",
       "4   What percentage of line managers strongly disa...   \n",
       "5           What contributed to the Windrush scandal?   \n",
       "6   What are the delivery and administrative power...   \n",
       "7   What is the primary focus of Reform's new prog...   \n",
       "8   What percentage of respondents strongly or som...   \n",
       "9   What was the rank of the UK in terms of measle...   \n",
       "10                         How does Reform define AI?   \n",
       "11  Is there a risk of a 'race to the bottom' in d...   \n",
       "\n",
       "                                             response  \\\n",
       "0   The \"General Power of Competence\" was introduc...   \n",
       "1   The causes for concern in England's local gove...   \n",
       "2   According to the Reform/CSW survey, 41% of lin...   \n",
       "3   According to the response to the FOI request, ...   \n",
       "4   According to the Reform /CSW survey, 41% of li...   \n",
       "5   The Windrush scandal was primarily contributed...   \n",
       "6   Local government in England has various delive...   \n",
       "7   The primary focus of Reform's new programme, \"...   \n",
       "8   The provided content does not include informat...   \n",
       "9   In 2021, the UK ranked 31st in terms of measle...   \n",
       "10  Reform uses the definition of AI provided by t...   \n",
       "11  Yes, there are concerns that more granular dec...   \n",
       "\n",
       "                                            reference  context_recall  \\\n",
       "0   The \"General Power of Competence,\" as outlined...            0.00   \n",
       "1   The main concerns with England's local governm...            1.00   \n",
       "2                                                0.41            1.00   \n",
       "3   The response to the FOI request is not clear o...            1.00   \n",
       "4                                                0.21            1.00   \n",
       "5   “Operational and organisational failing” at th...            0.00   \n",
       "6   In England, local governments possess specific...            0.00   \n",
       "7   The primary focus of Reform's new programme, \"...            0.75   \n",
       "8                                         62 per cent            0.00   \n",
       "9                                                31st            1.00   \n",
       "10  Theories and techniques developed to allow com...            1.00   \n",
       "11  There is a concern that increased decentralisa...            1.00   \n",
       "\n",
       "    factual_correctness  faithfulness  semantic_similarity  \n",
       "0                  0.69      0.000000             0.971981  \n",
       "1                  0.76      0.904762             0.975568  \n",
       "2                   NaN      1.000000             0.777819  \n",
       "3                  0.67      1.000000             0.903955  \n",
       "4                   NaN      0.500000             0.762745  \n",
       "5                  0.00      0.040000             0.784868  \n",
       "6                  0.44      0.060606             0.962029  \n",
       "7                  0.74      1.000000             0.982347  \n",
       "8                   NaN      0.000000             0.783762  \n",
       "9                   NaN      1.000000             0.769668  \n",
       "10                 0.00      1.000000             0.869919  \n",
       "11                 0.82      1.000000             0.977776  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results[[\"user_input\", \"response\", \"reference\", \"context_recall\", \"factual_correctness\", \"faithfulness\", \"semantic_similarity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Context Recall: 0.65\n",
      "Factual Correctness: 0.34\n",
      "Faithfulness: 0.63\n",
      "Semantic Similarity: 0.88\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary:\")\n",
    "round_up = 2\n",
    "\n",
    "context_recall = eval_results[\"context_recall\"].values\n",
    "cr_avg = sum(context_recall)/len(context_recall)\n",
    "print(f\"Context Recall: {round(cr_avg, round_up)}\")\n",
    "\n",
    "factual_correctness = eval_results[\"factual_correctness\"].fillna(0).values\n",
    "fc_avg = sum(factual_correctness)/len(factual_correctness)\n",
    "print(f\"Factual Correctness: {round(fc_avg, round_up)}\")\n",
    "\n",
    "faithfulness = eval_results[\"faithfulness\"].values\n",
    "ff_avg = sum(faithfulness)/len(faithfulness)\n",
    "print(f\"Faithfulness: {round(ff_avg, round_up)}\")\n",
    "\n",
    "semantic_similarity = eval_results[\"semantic_similarity\"].values\n",
    "ss_avg = sum(semantic_similarity)/len(semantic_similarity)\n",
    "print(f\"Semantic Similarity: {round(ss_avg, round_up)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
